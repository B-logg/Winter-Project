import os
import cv2
import json
import torch
import argparse
import re
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, CLIPImageProcessor
from model.GLaMM import GLaMMForCausalLM
from model.llava import conversation as conversation_lib
from model.llava.mm_utils import tokenizer_image_token
from model.SAM.utils.transforms import ResizeLongestSide
from tools.utils import DEFAULT_IM_END_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX
from eval.utils import mask_to_rle_pytorch, coco_encode_rle

def parse_args():
    parser = argparse.ArgumentParser(description="GLaMM Test with Loss")
    parser.add_argument("--hf_model_path", required=True, help="Path to checkpoint")
    parser.add_argument("--test_json_path", required=True, help="Path to test.json")
    parser.add_argument("--image_folder", required=True, help="Image folder")
    parser.add_argument("--output_dir", required=True, help="Result save dir")
    parser.add_argument("--conv_type", default="llava_v1")
    return parser.parse_args()

class ForestTestDataset(Dataset):
    def __init__(self, json_path, image_folder, tokenizer, image_processor, transform, model_config):
        with open(json_path, 'r') as f: self.data = json.load(f)
        self.image_folder = image_folder
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.transform = transform
        self.seg_token_idx = tokenizer("[SEG]", add_special_tokens=False).input_ids[0]
        
    def __len__(self): return len(self.data)

    def preprocess_image(self, image_path):
        image_np = cv2.imread(image_path)
        image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        orig_size = image_np.shape[:2]
        
        # CLIP Image
        image_clip = self.image_processor.preprocess(image_np, return_tensors="pt")["pixel_values"][0]
        
        # SAM Image
        image_sam = self.transform.apply_image(image_np)
        resize_shape = image_sam.shape[:2]
        image_sam = torch.from_numpy(image_sam).permute(2, 0, 1).float()
        
        # SAM Normalization
        pixel_mean = torch.tensor([123.675, 116.28, 103.53]).view(3, 1, 1)
        pixel_std = torch.tensor([58.395, 57.12, 57.375]).view(3, 1, 1)
        image_sam = (image_sam - pixel_mean) / pixel_std
        
        return image_clip, image_sam, orig_size, resize_shape

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = os.path.join(self.image_folder, item['image'])
        
        # 1. ì´ë¯¸ì§€ ì²˜ë¦¬
        clip_img, sam_img, orig_size, resize_shape = self.preprocess_image(image_path)
        
        # 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Loss ê³„ì‚°ìš©: ì§ˆë¬¸+ë‹µë³€ / ì¶”ë¡ ìš©: ì§ˆë¬¸ë§Œ)
        human_q = item['conversations'][0]['value']
        gpt_a = item['conversations'][1]['value'] # GT Answer
        
        # --- Loss ê³„ì‚°ì„ ìœ„í•œ Full Prompt (Teacher Forcing) ---
        conv = conversation_lib.conv_templates["llava_v1"].copy()
        conv.messages = []
        
        # ì§ˆë¬¸ êµ¬ì„±
        q_text = f"The {DEFAULT_IMAGE_TOKEN} provides an overview of the picture.\n" + human_q # ìˆ˜ì • í•„ìš”
        
        q_text = q_text.replace(DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN)
        conv.append_message(conv.roles[0], q_text)
        conv.append_message(conv.roles[1], gpt_a) # ë‹µë³€ í¬í•¨
        full_prompt = conv.get_prompt()
        
        input_ids_loss = tokenizer_image_token(full_prompt, self.tokenizer, return_tensors='pt')

        """ í† í° í„°ì§ ë””ë²„ê¹…
        # =========================================================
        if input_ids_loss.shape[0] > 1536:
            print(f"\n[ğŸš¨ í† í° í­ë°œ ë°œê²¬!] ì´ í† í° ìˆ˜: {input_ids_loss.shape[0]}")
            print(f"ë¬¸ì œì˜ íŒŒì¼ëª…: {item['image']}")
            print(f"ë¬¸ì œì˜ í…ìŠ¤íŠ¸:\n{full_prompt}\n" + "="*50)
            # í™•ì¸ì„ ìœ„í•´ ì—¬ê¸°ì„œ í”„ë¡œê·¸ë¨ì„ ê°•ì œë¡œ ë©ˆì¶¥ë‹ˆë‹¤.
            raise ValueError("í† í° ê¸¸ì´ ì´ˆê³¼ ë°ì´í„°ë¥¼ ë°œê²¬í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        # =========================================================
        """

        if input_ids_loss.shape[0] > 1536:
            print(f"\n[ê²½ê³ ] ë°ì´í„° ìŠ¤í‚µ! (í† í°: {input_ids_loss.shape[0]}) -> {item['image']}")
            
            # 1. ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì•„ì£¼ ì§§ì€ ê°€ì§œ ì§ˆë¬¸ê³¼ ë‹µë³€([SEG] 1ê°œ í¬í•¨)ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.
            human_q = "íƒ„ì†Œ ì €ì¥ëŸ‰ì„ ë¶„ì„í•´ì¤˜."
            gpt_a = "ì‚°ë¦¼ì´ ê³¼ë°€í•˜ê±°ë‚˜ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆê· í˜•í•  ê²½ìš° ë‚˜ë¬´ì˜ ì•ˆì •ì„±ê³¼ ìƒìœ¡ íš¨ìœ¨ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°€ë„ ì¡°ì ˆ(ì˜ˆ: ì†ì•„ë² ê¸°)ì„ í†µí•´ ê±´ê°•ì„±ê³¼ íƒ„ì†Œ í¡ìˆ˜ ëŠ¥ë ¥ì„ ê°œì„ í•  í•„ìš”ê°€ ìˆë‹¤. [SEG]"
            
            # 2. ê°€ì§œ ë°ì´í„°ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ì‹œ ì§­ë‹ˆë‹¤.
            conv = conversation_lib.conv_templates["llava_v1"].copy()
            conv.messages = []

            q_text = DEFAULT_IMAGE_TOKEN + "\n" + human_q
            conv.append_message(conv.roles[0], q_text)
            conv.append_message(conv.roles[1], gpt_a)
            full_prompt = conv.get_prompt()
            
            # 3. í† í°ì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤. (ì´ì œ 1536ì„ ì ˆëŒ€ ë„˜ì§€ ì•ŠìŒ)
            input_ids_loss = tokenizer_image_token(full_prompt, self.tokenizer, return_tensors='pt')
        
        # --- Labels ìƒì„± (Human ì§ˆë¬¸ ë¶€ë¶„ì€ ë§ˆìŠ¤í‚¹ -100) ---
        labels = input_ids_loss.clone()
        # ê°„ë‹¨íˆ: "ASSISTANT:" ì´ì „ê¹Œì§€ëŠ” ëª¨ë‘ ë§ˆìŠ¤í‚¹ (-100)
        sep = "ASSISTANT: "
        parts = full_prompt.split(sep)
        if len(parts) >= 2:
            len_context = len(tokenizer_image_token(parts[0] + sep, self.tokenizer))
            labels[:len_context-1] = -100 # -1 ë¹¼ëŠ”ê±´ ì˜¤ì°¨ ë²”ìœ„ ë³´ì •
        
        # 3. GT ë§ˆìŠ¤í¬ ë¡œë“œ (Loss ê³„ì‚°ìš©)
        gt_mask = torch.zeros((1024, 1024)).float()
        mask_path = item.get('mask_path', None)
        if mask_path:
            if isinstance(mask_path, str): mask_path = [mask_path]
            for mp in mask_path:
                m = cv2.imread(os.path.join(self.image_folder, mp), 0)
                if m is not None:
                    m = cv2.resize(m, (1024, 1024), interpolation=cv2.INTER_NEAREST)
                    gt_mask = torch.maximum(gt_mask, torch.from_numpy(m).float())
        gt_mask = (gt_mask > 0).float().unsqueeze(0) # (1, 1024, 1024)

        return {
            "id": item['id'],
            "image_path": image_path,
            "human_q": human_q,
            "clip_img": clip_img,
            "sam_img": sam_img,
            "input_ids_loss": input_ids_loss,
            "labels": labels,
            "masks": gt_mask,
            "orig_size": orig_size,
            "resize_shape": resize_shape
        }

def main():
    from peft import PeftModel
    import torch
    import re

    args = parse_args()
    
    # ì›ë³¸ ëª¨ë¸ ê²½ë¡œ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í™•ì¸)
    BASE_MODEL_PATH = "checkpoints/GLaMM-GCG"

    # 1. ëª¨ë¸ ë¡œë“œ (Base Model + LoRA + Non-LoRA ë³‘í•©)
    print(f"Loading Base Model from {BASE_MODEL_PATH}...")
    
    # (1) í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False)
    tokenizer.pad_token = tokenizer.unk_token
    seg_token_idx = tokenizer("[SEG]", add_special_tokens=False).input_ids[0]
    
    # (2) ë² ì´ìŠ¤ ëª¨ë¸ ë¼ˆëŒ€ ë¶ˆëŸ¬ì˜¤ê¸°
    model = GLaMMForCausalLM.from_pretrained(
        BASE_MODEL_PATH, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, seg_token_idx=seg_token_idx,
        train_mask_decoder=True 
    )
    
    # (3) LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
    print(f"Applying LoRA weights from {args.hf_model_path}...")
    model = PeftModel.from_pretrained(model, args.hf_model_path)
    model = model.merge_and_unload() 
    
    # (4) Non-LoRA ê°€ì¤‘ì¹˜ ë®ì–´ì”Œìš°ê¸°
    non_lora_path = os.path.join(args.hf_model_path, 'non_lora_trainables.bin')
    if os.path.exists(non_lora_path):
        print(f"Loading non-LoRA trainables from {non_lora_path}...")
        non_lora_trainables = torch.load(non_lora_path, map_location='cpu')
        
        cleaned_state_dict = {}
        for k, v in non_lora_trainables.items():
            if k.startswith('base_model.model.'):
                cleaned_state_dict[k[17:]] = v
            else:
                cleaned_state_dict[k] = v
        model.load_state_dict(cleaned_state_dict, strict=False)

    model = model.cuda()
    model = model.bfloat16() # ì „ì²´ 1ì°¨ ë³€í™˜

    print("âœ… Setting Loss weights...")
    model.ce_loss_weight = 1.0
    model.dice_loss_weight = 0.5
    model.bce_loss_weight = 2.0

    # ëª¨ë¸ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë° ë²„í¼ ì „ìˆ˜ ì¡°ì‚¬ ê°•ì œ ìºìŠ¤íŒ…
    for name, param in model.named_parameters():
        if param.is_floating_point() and param.dtype != torch.bfloat16:
            param.data = param.data.to(torch.bfloat16)
    for name, buffer in model.named_buffers():
        if buffer.is_floating_point() and buffer.dtype != torch.bfloat16:
            buffer.data = buffer.data.to(torch.bfloat16)

    # ë¹„ì „ íƒ€ì›Œ ë° ê·¸ë¼ìš´ë”© ì¸ì½”ë” ê°œë³„ ê°•ì œ ìºìŠ¤íŒ…
    vision_tower = model.get_model().get_vision_tower()
    vision_tower.to(dtype=torch.bfloat16, device='cuda')
    model.get_model().grounding_encoder.to(dtype=torch.bfloat16, device='cuda')

    # âœ… [Monkey Patch] SAM Mask Decoder ì…êµ¬ ë´‰ì‡„ (Train ì½”ë“œì˜ í•µì‹¬ ë¡œì§ ì´ì‹)
    print("âœ… Applying Monkey Patch to SAM Mask Decoder (Force BF16 Inputs)...")
    base_glamm = model.get_model()
    if hasattr(base_glamm, "grounding_encoder"):
        mask_decoder = base_glamm.grounding_encoder.mask_decoder
        original_forward = mask_decoder.forward
        
        def mask_decoder_forward_wrapper(*args, **kwargs):
            new_args = [a.to(torch.bfloat16) if isinstance(a, torch.Tensor) and torch.is_floating_point(a) else a for a in args]
            new_kwargs = {k: (v.to(torch.bfloat16) if isinstance(v, torch.Tensor) and torch.is_floating_point(v) else v) for k, v in kwargs.items()}
            return original_forward(*new_args, **new_kwargs)
        
        mask_decoder.forward = mask_decoder_forward_wrapper
    # ---------------------------------------------------------------------

    clip_processor = CLIPImageProcessor.from_pretrained(model.config.vision_tower)
    transform = ResizeLongestSide(1024)

    # 2. ë°ì´í„°ì…‹ (num_workers=0 ê¶Œì¥: ë””ë²„ê¹… ë° ê²½ë¡œ ì¶©ëŒ ë°©ì§€)
    dataset = ForestTestDataset(args.test_json_path, args.image_folder, tokenizer, clip_processor, transform, model.config)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)

    os.makedirs(args.output_dir, exist_ok=True)
    
    # 3. í…ŒìŠ¤íŠ¸ ë£¨í”„
    total_loss = 0.0
    ce_loss = 0.0
    mask_loss = 0.0
    count = 0
    
    print(">>> Starting Test Loop (Loss Calculation & Inference)...")
    results = []
    
    for batch in tqdm(dataloader):
        print(f"\n[ë¡œê·¸] í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì´ë¯¸ì§€: {batch['id'][0]}")
        # ë°ì´í„° ì¤€ë¹„ (bfloat16)
        images = batch['clip_img'].cuda().bfloat16()
        sam_images = batch['sam_img'].cuda().bfloat16()
        input_ids_loss = batch['input_ids_loss'].cuda()
        labels = batch['labels'].cuda()
        gt_masks = batch['masks'].cuda().bfloat16()
        
        # (A) Loss Calculation
        resize_shape_list = [[batch['resize_shape'][0].item(), batch['resize_shape'][1].item()]]

        with torch.no_grad():
            outputs = model(
                input_ids=input_ids_loss,
                labels=labels,
                images=images,
                global_enc_images=images, # ëª…ì‹œì  ì „ë‹¬
                grounding_enc_images=sam_images,
                bboxes=None,
                attention_masks=None,
                masks_list=[gt_masks[0]], # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬
                label_list=[gt_masks[0]],
                resize_list=resize_shape_list,
                # Batch 1ì¼ ë•Œ offset ë³´ì •
                offset=torch.tensor([0, 1]).long().cuda() if input_ids_loss.shape[0] == 1 else None 
            )
            
            if 'loss' in outputs:
                total_loss += outputs['loss'].item()
                count += 1
            if 'ce_loss' in outputs: 
                ce_loss += outputs['ce_loss'].item()
            if 'mask_loss' in outputs: 
                mask_loss += outputs['mask_loss'].item()

        # (B) Inference (Generate)

        """
        human_q = batch['human_q'][0]

        if len(human_q) > 1000:
            human_q = "ì´ í•­ê³µ ì‚¬ì§„ì—ì„œ ë¶‰ì€ ë°•ìŠ¤ë¡œ í‘œì‹œëœ ë‚˜ë¬´ë“¤ì˜ íƒ„ì†Œ ì €ì¥ëŸ‰ì„ ë¶„ì„í•´ì¤˜."

        conv = conversation_lib.conv_templates[args.conv_type].copy()
        conv.messages = []
        q_text = f"The {DEFAULT_IMAGE_TOKEN} provides an overview of the picture.\n" + human_q
        q_text = q_text.replace(DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN)
        conv.append_message(conv.roles[0], q_text)
        conv.append_message(conv.roles[1], "")
        prompt = conv.get_prompt()
        
        input_ids_gen = tokenizer_image_token(prompt, tokenizer, return_tensors='pt').unsqueeze(0).cuda()
        
        orig_size = [batch['orig_size'][0].numpy(), batch['orig_size'][1].numpy()]
        resize_shape = [batch['resize_shape'][0].numpy(), batch['resize_shape'][1].numpy()]
        
        # ëª¨ë¸ ì¶”ë¡  (evaluate ë©”ì†Œë“œ ì‚¬ìš©)
        output_ids, pred_masks = model.evaluate(
            images, sam_images, input_ids_gen, [resize_shape], [orig_size],
            max_tokens_new=512, bboxes=None
        )
        
        # ê²°ê³¼ íŒŒì‹±
        out_ids = output_ids[0][output_ids[0] != IMAGE_TOKEN_INDEX]
        text_out = tokenizer.decode(out_ids, skip_special_tokens=False).split("ASSISTANT: ")[-1]
        cleaned_text = re.sub(r'<.*?>', '', text_out).replace('[SEG]', '').strip()
        
        rle_masks = []
        if pred_masks is not None and len(pred_masks) > 0:
            pred_masks_tensor = pred_masks[0].cpu() > 0
            rle_masks = [coco_encode_rle(m) for m in mask_to_rle_pytorch(pred_masks_tensor)]
        
        results.append({
            "image_id": batch['id'][0],
            "caption": cleaned_text,
            "pred_masks": rle_masks
        })

    """

    # ìµœì¢… ê²°ê³¼ ë³´ê³  ë° ì €ì¥
    if count > 0:
        print("\n" + "="*30)
        print(f" [TEST SET LOSS REPORT]")
        print(f" - Total Loss: {total_loss / count:.4f}")
        print(f" - CE Loss (Text): {ce_loss / count:.4f}")
        print(f" - Mask Loss (Seg): {mask_loss / count:.4f}")
        print("="*30 + "\n")
    
    save_path = os.path.join(args.output_dir, "test_predictions.json")
    with open(save_path, 'w') as f:
        json.dump(results, f)
    print(f"Predictions saved to {save_path}")

if __name__ == "__main__":
    main()