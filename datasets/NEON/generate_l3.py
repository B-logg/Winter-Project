import json
import os
import cv2
import numpy as np
import pandas as pd
import rasterio
import torch
import requests
from PIL import Image
import google.generativeai as genai
from segment_anything_hq import sam_model_registry, SamPredictor

from prepare_l3 import neon_l2_bridge

GEMINI_API_KEY = os.getenv("Gemini_API_KEY")
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__)) # í˜„ì¬ ê²½ë¡œ(ìƒëŒ€)
INPUT_PATH = os.path.join(CURRENT_DIR, "NEON_dataset.csv")
OUTPUT_PATH = os.path.join(CURRENT_DIR, "l3_dataset")
SAM_CHECKPOINT = os.path.join(CURRENT_DIR, "checkpoints", "sam_hq_vit_l.pth")
device = "cuda" if torch.cuda.is_available() else "cpu"
NEON_PRODUCT_ID = "DP3.30010.001"

# Gemini API ì„¤ì • ë° SAM ì„¤ì •
def init_models():
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-2.5-pro') # ì •êµí•œ CoTë¥¼ ìœ„í•´ pro ëª¨ë¸ ì‚¬ìš©
    sam = sam_model_registry["vit_l"](checkpoint=SAM_CHECKPOINT)
    sam.to(device=device)
    predictor = SamPredictor(sam)
    return gemini_model, predictor

def download_neon_image(site, year, tile_id, save_dir):
    """
    NEON API êµ¬ì¡° ë³€ê²½: Data API ëŒ€ì‹  Product APIë¡œ ê°€ìš© ì›”(Month)ì„ ë¨¼ì € ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    filename = f"{tile_id}.tif"
    save_path = os.path.join(save_dir, filename)
    
    if os.path.exists(save_path):
        return save_path
    
    # tile_id íŒŒì‹± (ì•ˆì „ì¥ì¹˜)
    try:
        parts = tile_id.split('_')
        safe_year = parts[0]
        safe_site = parts[1]
    except:
        return None

    print(f"ğŸ” Searching NEON API for: {filename} ({safe_site}, {safe_year})...")

    # 1. Product APIë¥¼ í†µí•´ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ê°€ìš© ì›”(Month) ì¡°íšŒ
    # Data APIëŠ” ì›”(Month) ì—†ì´ í˜¸ì¶œí•˜ë©´ 400 ì—ëŸ¬ê°€ ëœ¸!
    product_url = f"https://data.neonscience.org/api/v0/products/{NEON_PRODUCT_ID}"
    
    try:
        r = requests.get(product_url)
        if r.status_code != 200:
            print(f"Product API Error: {r.status_code}")
            return None
        
        data = r.json()
        if 'data' not in data: return None

        # í•´ë‹¹ ì‚¬ì´íŠ¸(BART ë“±) ì •ë³´ ì°¾ê¸°
        site_info = next((s for s in data['data']['siteCodes'] if s['siteCode'] == safe_site), None)
        
        if not site_info:
            print(f"Site {safe_site} not found in product {NEON_PRODUCT_ID}")
            return None
        
        # í•´ë‹¹ ì—°ë„(year)ê°€ í¬í•¨ëœ ì›”ë§Œ í•„í„°ë§ (ì˜ˆ: "2022-06")
        available_months = [m for m in site_info['availableMonths'] if m.startswith(str(safe_year))]
        
        if not available_months:
            print(f"No data found for {safe_site} in {safe_year}")
            return None

        # 2. ê° ì›”ë³„ Data APIë¥¼ ì¡°íšŒí•˜ì—¬ íŒŒì¼ URL ì°¾ê¸°
        file_url = None
        for month in sorted(available_months, reverse=True):
            # ì´ì œ ì •í™•í•œ ì›”(YYYY-MM)ì„ ì•Œì•˜ìœ¼ë‹ˆ Data API í˜¸ì¶œ ê°€ëŠ¥
            data_url = f"https://data.neonscience.org/api/v0/data/{NEON_PRODUCT_ID}/{safe_site}/{month}"
            r_files = requests.get(data_url).json()
            
            if 'data' not in r_files or 'files' not in r_files['data']:
                continue

            for file_info in r_files['data']['files']:
                if file_info['name'] == filename:
                    file_url = file_info['url']
                    print(f"Found URL in {month}")
                    break
            if file_url: break
        
        if not file_url:
            print(f"File not found in NEON database: {filename}")
            return None

        # 3. ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
        print(f"â¬‡ï¸ Downloading...")
        with requests.get(file_url, stream=True) as r:
            r.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print("Download Complete.")
        return save_path

    except Exception as e:
        print(f"Download Error: {e}")
        return None


def get_pixel_coords(tif_path, utm_bbox):
    """
    GeoTIFFì˜ Transform ì •ë³´ë¥¼ ì´ìš©í•´ UTM ì¢Œí‘œ -> Pixel ì¢Œí‘œ ë³€í™˜
    utm_bbox: [min_x, min_y, max_x, max_y] (ì§€ë„ ì¢Œí‘œ)
    returns: [min_x, min_y, max_x, max_y] (í”½ì…€ ì¢Œí‘œ)
    """
    with rasterio.open(tif_path) as src:
        # rasterio.indexëŠ” (row, col) = (y, x)ë¥¼ ë°˜í™˜í•¨ì— ì£¼ì˜!
        # min_x, max_y (Top-Left)
        row_min, col_min = src.index(utm_bbox[0], utm_bbox[3]) 
        # max_x, min_y (Bottom-Right)
        row_max, col_max = src.index(utm_bbox[2], utm_bbox[1])
        
        h, w = src.shape
        
        # ìŒìˆ˜ ì¢Œí‘œ ë°©ì§€ ë° ì´ë¯¸ì§€ í¬ê¸° ë‚´ë¡œ í´ë¦¬í•‘
        x1 = max(0, min(col_min, w))
        y1 = max(0, min(row_min, h))
        x2 = max(0, min(col_max, w))
        y2 = max(0, min(row_max, h))
        
        # [min_x, min_y, max_x, max_y] ìˆœì„œë¡œ ë°˜í™˜
        return [min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)], w, h
    
def normalize_bbox(px_bbox, w, h):
    """ í”½ì…€ ì¢Œí‘œ -> 0~1000 ì •ê·œí™” ì¢Œí‘œ"""
    return [
        int(px_bbox[1] / h * 1000), # y_min
        int(px_bbox[0] / w * 1000), # x_min
        int(px_bbox[3] / h * 1000), # y_max
        int(px_bbox[2] / w * 1000)  # x_max
    ]

def process_dataset(df, model, predictor):
    os.makedirs(os.path.join(OUTPUT_PATH, "images"), exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_PATH, "masks"), exist_ok=True)
    temp_dir = os.path.join(OUTPUT_PATH, "temp_tif")
    os.makedirs(temp_dir, exist_ok=True)
    
    l3_results = []
    
    # ì´ë¯¸ Tile IDë¡œ ìœ ë‹ˆí¬í•˜ë¯€ë¡œ groupbyê°€ ì‚¬ì‹¤ìƒ 1ê°œì”© ì²˜ë¦¬í•¨
    grouped = df.groupby('tile_id')
    print(f"Processing {len(grouped)} tiles...")

    for idx, (tile_id, group) in enumerate(grouped):
        # ê·¸ë£¹ì—ëŠ” rowê°€ 1ê°œë§Œ ìˆë‹¤ê³  ê°€ì • (prepare_neon êµ¬ì¡°ìƒ)
        row = group.iloc[0] 
        site = row['site']
        year = row['year']
        
        tif_path = download_neon_image(site, year, tile_id, temp_dir)
        if not tif_path: continue

        try:
            with rasterio.open(tif_path) as src:
                img_array = src.read([1, 2, 3]) 
                img_array = np.moveaxis(img_array, 0, -1)
            
            predictor.set_image(img_array)
            pil_img = Image.fromarray(img_array)
            
            jpg_filename = f"{tile_id}.jpg"
            pil_img.save(os.path.join(OUTPUT_PATH, "images", jpg_filename), quality=85)

            bboxes_list = eval(row['bboxes']) if isinstance(row['bboxes'], str) else row['bboxes']
            heights_list = eval(row['individual_heights']) if isinstance(row['individual_heights'], str) else row['individual_heights']
            
            areas_list = eval(row['individual_crown_areas']) if isinstance(row['individual_crown_areas'], str) else row['individual_crown_areas']
            dbhs_list = eval(row['individual_dbhs']) if isinstance(row['individual_dbhs'], str) else row['individual_dbhs']
            types_list = eval(row['individual_tree_types']) if isinstance(row['individual_tree_types'], str) else row['individual_tree_types']

            # ì´ë¯¸ì§€ ë‚´ì˜ ë‚˜ë¬´ ìˆ˜ë§Œí¼ ë°˜ë³µ
            for i, utm_box in enumerate(bboxes_list):
                try:
                    # utm_boxëŠ” ì´ì œ [x, y, x, y] ìˆ«ì 4ê°œì„!
                    px_box, w, h = get_pixel_coords(tif_path, utm_box)
                    
                    # ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ ìŠ¤í‚µ
                    if (px_box[2] - px_box[0]) < 3 or (px_box[3] - px_box[1]) < 3: continue

                    norm_box = normalize_bbox(px_box, w, h)

                    # Bridgeìš© ë°ì´í„° í¬ì¥ (ë‹¨ì¼ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ì „ë‹¬)
                    wrapped_row = {
                        'site': site,
                        'tile_id': tile_id,
                        'bboxes': [utm_box], 
                        'individual_heights': [heights_list[i]],
                        'individual_crown_areas': [areas_list[i]], 
                        'individual_dbhs': [dbhs_list[i]], 
                        'individual_tree_types': [types_list[i]]
                    }
                    
                    # tree_idxëŠ” ë¬´ì¡°ê±´ 0 (wrapped_rowì— 1ê°œë§Œ ë„£ì—ˆìœ¼ë¯€ë¡œ)
                    l3_data = neon_l2_bridge(wrapped_row, tree_idx=0, custom_bbox=norm_box)
                    
                    # Gemini
                    response = model.generate_content([l3_data['prompt'], pil_img])
                    clean_text = response.text.replace('```json', '').replace('```', '').strip()
                    try: res_json = json.loads(clean_text)
                    except: res_json = {"dense_caption": clean_text}

                    # SAM-H
                    input_box = np.array(px_box)
                    masks, _, _ = predictor.predict(box=input_box[None, :], multimask_output=False)
                    
                    mask_filename = f"mask_{l3_data['id']}.png"
                    cv2.imwrite(os.path.join(OUTPUT_PATH, "masks", mask_filename), (masks[0] * 255).astype(np.uint8))

                    final_entry = {
                        "id": l3_data['id'],
                        "image": jpg_filename,
                        "conversations": [
                            {"from": "human", "value": l3_data['human_query']},
                            {"from": "gpt", "value": res_json.get('dense_caption', "")}
                        ],
                        "mask_path": f"masks/{mask_filename}",
                        "bbox_normalized": norm_box 
                    }
                    l3_results.append(final_entry)
                    print(f"  Saved Tree: {l3_data['id']}")

                except Exception as e:
                    print(f"  Individual Tree Error: {e}")
                    continue
        
        except Exception as e:
            print(f"Tile Error: {e}")
        
        finally:
            if os.path.exists(tif_path):
                os.remove(tif_path)
                print(f"Deleted temp file: {tif_path}")

    with open(os.path.join(OUTPUT_PATH, "l3_dataset.json"), "w", encoding='utf-8') as f:
        json.dump(l3_results, f, indent=4, ensure_ascii=False)
    print("All Done!")

if __name__ == "__main__":
    gemini, sam = init_models()
    df = pd.read_csv(INPUT_PATH)
    
    # [í…ŒìŠ¤íŠ¸ìš©] 5ê°œ íƒ€ì¼ë§Œ ì‹¤í–‰
    unique_tiles = df['tile_id'].unique()
    sample_tiles = np.random.choice(unique_tiles, min(len(unique_tiles), 5), replace=False)
    df = df[df['tile_id'].isin(sample_tiles)]
    
    process_dataset(df, gemini, sam)