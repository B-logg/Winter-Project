import torch
import transformers
from transformers import BitsAndBytesConfig
from peft import prepare_model_for_kbit_training
import argparse
import bitsandbytes as bnb

# ì‚¬ìš©ìë‹˜ì˜ ëª¨ë¸ íŒŒì¼ import
from model.GLaMM import GLaMMForCausalLM 

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--version", default="MBZUAI/GLaMM-GranD-Pretrained")
    # ê²½ë¡œ ê´€ë ¨ ì¸ìëŠ” ì‚¬ìš©ìë‹˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”í•˜ë©´ ìˆ˜ì •í•´ì£¼ì„¸ìš”
    parser.add_argument("--local_rank", default=0, type=int)
    return parser.parse_args()

def find_all_linear_names(model):
    cls = torch.nn.Linear
    lora_module_names = set()
    # 4ë¹„íŠ¸ ë ˆì´ì–´ í´ë˜ìŠ¤ë„ í™•ì¸
    import bitsandbytes as bnb
    
    print("\n[Diagnostic] Searching for Linear layers...")
    for name, module in model.named_modules():
        # ëª¨ë“ˆì˜ ì§„ì§œ íƒ€ì…ì„ í™•ì¸
        if isinstance(module, cls) or isinstance(module, bnb.nn.Linear4bit):
            names = name.split('.')
            lora_module_names.add(names[-1])
            
            # ìƒ˜í”Œë¡œ ëª‡ ê°œë§Œ ìì„¸íˆ ì¶œë ¥ (ë„ˆë¬´ ë§ìœ¼ë‹ˆê¹Œ)
            if "grounding_encoder" in name or "layers.0.self_attn" in name:
                print(f"  Found: {name} | Type: {type(module)} | Dtype: {module.weight.dtype}")
                
    if 'lm_head' in lora_module_names: 
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

def main():
    args = parse_args()
    torch.cuda.set_device(args.local_rank)
    
    print(f"Loading GLaMM from {args.version}...")
    
    # 1. 4-bit ë¡œë“œ ì„¤ì • (ì‚¬ìš©ìë‹˜ ì½”ë“œì™€ ë™ì¼)
    skip_modules = ["vision_tower", "grounding_encoder", "mm_projector", 
                    "text_hidden_fcs", "region_encoder", "lm_head", "embed_tokens"]
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        llm_int8_skip_modules=skip_modules
    )
    
    # 2. ëª¨ë¸ ë¡œë“œ
    model = GLaMMForCausalLM.from_pretrained(
        args.version,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        device_map = {"": args.local_rank},
        # í•„ìˆ˜ ì¸ìë“¤ ë”ë¯¸ë¡œ ì±„ì›€
        train_mask_decoder=True, out_dim=256,
        ce_loss_weight=1.0, dice_loss_weight=0.5, bce_loss_weight=2.0,
        seg_token_idx=123, vision_pretrained="./checkpoints/sam_vit_h_4b8939.pth",
        vision_tower="openai/clip-vit-large-patch14-336",
        use_mm_start_end=True, mm_vision_select_layer=-2, with_region=True
    )
    
    # 3. ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ” [1] ì „ì²´ ëª¨ë¸ êµ¬ì¡° ìš”ì•½ (Top-level modules)")
    print("="*50)
    for name, module in model.named_children():
        print(f"[{name}]: {type(module)}")
        
    print("\n" + "="*50)
    print("ğŸ” [2] SAM (Grounding Encoder) ë‚´ë¶€ êµ¬ì¡° í™•ì¸")
    print("="*50)
    if hasattr(model.model, "grounding_encoder"):
        sam = model.model.grounding_encoder
        print(f"SAM Type: {type(sam)}")
        # SAM ë‚´ë¶€ì˜ ì²« ë²ˆì§¸ ë¸”ë¡ë§Œ ì°ì–´ì„œ êµ¬ì¡° í™•ì¸
        for name, mod in sam.named_modules():
            if "layers.0" in name: 
                print(f" - {name} : {type(mod)}")
    else:
        print("âŒ SAM not found in model.model.grounding_encoder")

    print("\n" + "="*50)
    print("ğŸ” [3] find_target_linear_modules ê²°ê³¼ í™•ì¸")
    print("="*50)
    targets = find_all_linear_names(model)
    print(f"\nğŸ‘‰ Detected Target Modules: {targets}")
    
    print("\n" + "="*50)
    print("âœ… ì§„ë‹¨ ì™„ë£Œ. ìœ„ ë‚´ìš©ì„ ë³´ì—¬ì£¼ì„¸ìš”.")
    print("="*50)

if __name__ == "__main__":
    main()