nohup: ignoring input
bash: /shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/libtinfo.so.6: no version information available (required by bash)
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
[2026-02-05 00:13:32,467] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-05 00:13:36,030] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2026-02-05 00:13:36,067] [INFO] [runner.py:568:main] cmd = /shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_glamm.py --version /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG --dataset_path /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json --image_folder /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets --vision_pretrained /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth --output_dir /shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU --batch_size 2 --grad_accumulation_steps 3 --epochs 5 --lr 2e-4 --workers 8 --print_freq 1 --lora_r 128 --lora_alpha 256 --lora_dropout 0.05 --vision_tower openai/clip-vit-large-patch14-336 --use_mm_start_end --train_mask_decoder
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
[2026-02-05 00:13:37,641] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-05 00:13:40,078] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2026-02-05 00:13:40,078] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2026-02-05 00:13:40,078] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2026-02-05 00:13:40,078] [INFO] [launch.py:163:main] dist_world_size=4
[2026-02-05 00:13:40,078] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2026-02-05 00:13:40,080] [INFO] [launch.py:253:main] process 846775 spawned with command: ['/shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10', '-u', 'train_glamm.py', '--local_rank=0', '--version', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG', '--dataset_path', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json', '--image_folder', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets', '--vision_pretrained', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth', '--output_dir', '/shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU', '--batch_size', '2', '--grad_accumulation_steps', '3', '--epochs', '5', '--lr', '2e-4', '--workers', '8', '--print_freq', '1', '--lora_r', '128', '--lora_alpha', '256', '--lora_dropout', '0.05', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--use_mm_start_end', '--train_mask_decoder']
[2026-02-05 00:13:40,086] [INFO] [launch.py:253:main] process 846776 spawned with command: ['/shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10', '-u', 'train_glamm.py', '--local_rank=1', '--version', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG', '--dataset_path', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json', '--image_folder', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets', '--vision_pretrained', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth', '--output_dir', '/shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU', '--batch_size', '2', '--grad_accumulation_steps', '3', '--epochs', '5', '--lr', '2e-4', '--workers', '8', '--print_freq', '1', '--lora_r', '128', '--lora_alpha', '256', '--lora_dropout', '0.05', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--use_mm_start_end', '--train_mask_decoder']
[2026-02-05 00:13:40,087] [INFO] [launch.py:253:main] process 846777 spawned with command: ['/shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10', '-u', 'train_glamm.py', '--local_rank=2', '--version', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG', '--dataset_path', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json', '--image_folder', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets', '--vision_pretrained', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth', '--output_dir', '/shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU', '--batch_size', '2', '--grad_accumulation_steps', '3', '--epochs', '5', '--lr', '2e-4', '--workers', '8', '--print_freq', '1', '--lora_r', '128', '--lora_alpha', '256', '--lora_dropout', '0.05', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--use_mm_start_end', '--train_mask_decoder']
[2026-02-05 00:13:40,094] [INFO] [launch.py:253:main] process 846778 spawned with command: ['/shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10', '-u', 'train_glamm.py', '--local_rank=3', '--version', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG', '--dataset_path', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json', '--image_folder', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets', '--vision_pretrained', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth', '--output_dir', '/shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU', '--batch_size', '2', '--grad_accumulation_steps', '3', '--epochs', '5', '--lr', '2e-4', '--workers', '8', '--print_freq', '1', '--lora_r', '128', '--lora_alpha', '256', '--lora_dropout', '0.05', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--use_mm_start_end', '--train_mask_decoder']
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
[2026-02-05 00:13:42,481] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
[2026-02-05 00:13:42,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/accelerator/cuda_accelerator.py:34: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml
[2026-02-05 00:13:43,174] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-05 00:13:43,181] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Overriding tokenizer model_max_length to 8192
Loading GLaMM from /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Overriding tokenizer model_max_length to 8192
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading GLaMM from /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Overriding tokenizer model_max_length to 8192
Loading GLaMM from /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Overriding tokenizer model_max_length to 8192
Loading GLaMM from /shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG...
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.49s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.44s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 18.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 18.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.52s/it]
Casting modules to BF16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.19s/it]
Casting modules to BF16...
Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 18.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.91s/it]
Casting modules to BF16...
Casting modules to BF16...
Trainable Params: 662,957,284
Loading Dataset from /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2026-02-05 00:16:40,410] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2026-02-05 00:16:40,410] [INFO] [comm.py:637:init_distributed] cdb=None
Trainable Params: 662,957,284
Loading Dataset from /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json
Trainable Params: 662,957,284
Loading Dataset from /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2026-02-05 00:16:41,424] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2026-02-05 00:16:41,424] [INFO] [comm.py:637:init_distributed] cdb=None
Trainable Params: 662,957,284
Loading Dataset from /shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2026-02-05 00:16:41,703] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2026-02-05 00:16:41,703] [INFO] [comm.py:637:init_distributed] cdb=None
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2026-02-05 00:16:42,010] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2026-02-05 00:16:42,010] [INFO] [comm.py:637:init_distributed] cdb=None
[2026-02-05 00:16:42,010] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
libibverbs: Warning: couldn't open config directory '/croot/rdma-core_1754931709056/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho/etc/libibverbs.d'.
libibverbs: Warning: couldn't open config directory '/croot/rdma-core_1754931709056/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho/etc/libibverbs.d'.
libibverbs: Warning: couldn't open config directory '/croot/rdma-core_1754931709056/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho/etc/libibverbs.d'.
libibverbs: Warning: couldn't open config directory '/croot/rdma-core_1754931709056/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho/etc/libibverbs.d'.
Using /shared/home/naislab/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /shared/home/naislab/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[2026-02-05 00:16:46,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Detected CUDA files, patching ldflags
Emitting ninja build file /shared/home/naislab/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /shared/home/naislab/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /shared/home/naislab/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.0953676700592041 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10406064987182617 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10396742820739746 seconds
[2026-02-05 00:16:46,795] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2026-02-05 00:16:46,795] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Loading extension module fused_adam...
Time to load fused_adam op: 0.10444474220275879 seconds
[2026-02-05 00:16:47,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2026-02-05 00:16:47,609] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2026-02-05 00:16:47,609] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2026-02-05 00:16:47,609] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2026-02-05 00:16:47,609] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2026-02-05 00:16:47,609] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2026-02-05 00:16:47,609] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Starting Training Loop
Starting Training Loop
Starting Training Loop
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2026-02-05 00:17:01,627] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2026-02-05 00:17:01,643] [INFO] [utils.py:801:see_memory_usage] MA 6.73 GB         Max_MA 7.03 GB         CA 7.96 GB         Max_CA 8 GB 
[2026-02-05 00:17:01,643] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.43 GB, percent = 4.8%
[2026-02-05 00:17:03,310] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2026-02-05 00:17:03,310] [INFO] [utils.py:801:see_memory_usage] MA 6.73 GB         Max_MA 7.34 GB         CA 8.58 GB         Max_CA 9 GB 
[2026-02-05 00:17:03,311] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.89 GB, percent = 4.9%
[2026-02-05 00:17:03,311] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2026-02-05 00:17:04,398] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2026-02-05 00:17:04,399] [INFO] [utils.py:801:see_memory_usage] MA 6.73 GB         Max_MA 6.73 GB         CA 8.58 GB         Max_CA 9 GB 
[2026-02-05 00:17:04,399] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 50.11 GB, percent = 5.0%
[2026-02-05 00:17:04,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2026-02-05 00:17:04,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2026-02-05 00:17:04,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fd98b69a890>
[2026-02-05 00:17:04,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[[0.9, 0.95]]
[2026-02-05 00:17:04,537] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2026-02-05 00:17:04,537] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2026-02-05 00:17:04,537] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2026-02-05 00:17:04,537] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2026-02-05 00:17:04,537] [INFO] [config.py:1000:print]   amp_params ................... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd98b69a770>
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   dump_state ................... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2026-02-05 00:17:04,538] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   global_rank .................. 0
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 3
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.0, 'betas': [0.9, 0.95]}
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   pld_params ................... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 10815, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0002, 'warmup_num_steps': 100}
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   steps_per_print .............. 10
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   train_batch_size ............. 24
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2026-02-05 00:17:04,539] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2026-02-05 00:17:04,540] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   world_size ................... 4
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2026-02-05 00:17:04,572] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2026-02-05 00:17:04,572] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 3, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0002, 
            "weight_decay": 0.0, 
            "betas": [0.9, 0.95]
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 1.081500e+04, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0002, 
            "warmup_num_steps": 100
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "allgather_bucket_size": 5.000000e+08
    }
}
Starting Training Loop
/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Epoch 1/5:   0%|          | 0/2163 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 376, in <module>
    main()
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 335, in main
    outputs = model_engine(**batch)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/peft_model.py", line 918, in forward
    return self.base_model(
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 133, in forward
    return super().forward(**kwargs) if "past_key_values" in kwargs else self.model_forward(**kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 144, in model_forward
    output, output_hidden_states = self._training_path(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 200, in _training_path
    output = super().forward(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/model/language_model/llava_llama.py", line 90, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 134, in prepare_inputs_labels_for_multimodal
    image_features, image_forward_outs = self.encode_images(images)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 94, in encode_images
    image_features = proj(image_features)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)
Traceback (most recent call last):
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 376, in <module>
    main()
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 335, in main
    outputs = model_engine(**batch)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/peft_model.py", line 918, in forward
    return self.base_model(
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 133, in forward
    return super().forward(**kwargs) if "past_key_values" in kwargs else self.model_forward(**kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 144, in model_forward
    output, output_hidden_states = self._training_path(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 200, in _training_path
    output = super().forward(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/model/language_model/llava_llama.py", line 90, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 134, in prepare_inputs_labels_for_multimodal
    image_features, image_forward_outs = self.encode_images(images)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 94, in encode_images
    image_features = proj(image_features)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)
Traceback (most recent call last):
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 376, in <module>
    main()
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/train_glamm.py", line 335, in main
    outputs = model_engine(**batch)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/peft_model.py", line 918, in forward
    return self.base_model(
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 133, in forward
    return super().forward(**kwargs) if "past_key_values" in kwargs else self.model_forward(**kwargs)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 144, in model_forward
    output, output_hidden_states = self._training_path(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/GLaMM.py", line 200, in _training_path
    output = super().forward(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/model/language_model/llava_llama.py", line 90, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 134, in prepare_inputs_labels_for_multimodal
    image_features, image_forward_outs = self.encode_images(images)
  File "/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/model/llava/llava_with_region_arch.py", line 94, in encode_images
    image_features = proj(image_features)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/shared/home/naislab/학부연구생/bosung/my_envs/glamm/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)
[2026-02-05 00:18:52,406] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 846775
[2026-02-05 00:18:53,542] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 846776
[2026-02-05 00:18:53,542] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 846777
[2026-02-05 00:18:53,574] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 846778
[2026-02-05 00:18:54,038] [ERROR] [launch.py:322:sigkill_handler] ['/shared/home/naislab/학부연구생/bosung/my_envs/glamm/bin/python3.10', '-u', 'train_glamm.py', '--local_rank=3', '--version', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/GLaMM-GCG', '--dataset_path', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets/train.json', '--image_folder', '/shared/home/naislab/학부연구생/bosung/Winter-Project/datasets/datasets', '--vision_pretrained', '/shared/home/naislab/학부연구생/bosung/Winter-Project/groundingLMM/checkpoints/sam_vit_h_4b8939.pth', '--output_dir', '/shared/home/naislab/학부연구생/bosung/Winter-Project/checkpoints/GLaMM-Forest-A40-4GPU', '--batch_size', '2', '--grad_accumulation_steps', '3', '--epochs', '5', '--lr', '2e-4', '--workers', '8', '--print_freq', '1', '--lora_r', '128', '--lora_alpha', '256', '--lora_dropout', '0.05', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--use_mm_start_end', '--train_mask_decoder'] exits with return code = 1
